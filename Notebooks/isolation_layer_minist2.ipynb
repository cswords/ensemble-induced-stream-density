{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "!rm -rf ./logs/mnist2/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "image_w, image_h = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:  60000 n_classes: 10\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "n_classes = ds_info.features['label'].num_classes\n",
    "n = ds_info.splits['train'].num_examples\n",
    "print(\"n: \", n, \"n_classes:\", n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ds(ds):\n",
    "    def normalize_img(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        image = layers.Resizing(image_w, image_h)(image)\n",
    "        image = tf.reshape(image, [image_w * image_h])\n",
    "        # label = keras.utils.to_categorical(label)\n",
    "        label = tf.one_hot(tf.cast(label, tf.int32), n_classes)\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        return image, label\n",
    "\n",
    "    ds = ds.cache().map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_train = (\n",
    "    normalize_ds(ds_train).shuffle(n).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "ds_test = normalize_ds(ds_test).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(ds_train.unbatch().shuffle(n).take(psi).batch(psi).as_numpy_iterator())[0][0]\n",
    "        for _ in range(t)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 8s 5ms/step - loss: 0.0679 - acc: 0.9036 - val_loss: 0.0504 - val_acc: 0.9298\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0374 - acc: 0.9501 - val_loss: 0.0397 - val_acc: 0.9479\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0334 - acc: 0.9585 - val_loss: 0.0320 - val_acc: 0.9582\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0276 - acc: 0.9659 - val_loss: 0.0506 - val_acc: 0.9439\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0275 - acc: 0.9688 - val_loss: 0.0362 - val_acc: 0.9614\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0241 - acc: 0.9725 - val_loss: 0.0346 - val_acc: 0.9584\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0224 - acc: 0.9754 - val_loss: 0.0306 - val_acc: 0.9681\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0208 - acc: 0.9774 - val_loss: 0.0295 - val_acc: 0.9709\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0199 - acc: 0.9793 - val_loss: 0.0292 - val_acc: 0.9678\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0187 - acc: 0.9814 - val_loss: 0.0285 - val_acc: 0.9696\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0171 - acc: 0.9831 - val_loss: 0.0253 - val_acc: 0.9727\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0171 - acc: 0.9833 - val_loss: 0.0343 - val_acc: 0.9676\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0158 - acc: 0.9851 - val_loss: 0.0255 - val_acc: 0.9731\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0152 - acc: 0.9854 - val_loss: 0.0257 - val_acc: 0.9731\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0144 - acc: 0.9863 - val_loss: 0.0319 - val_acc: 0.9678\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0142 - acc: 0.9877 - val_loss: 0.0302 - val_acc: 0.9715\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0140 - acc: 0.9873 - val_loss: 0.0257 - val_acc: 0.9750\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0129 - acc: 0.9882 - val_loss: 0.0325 - val_acc: 0.9698\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0129 - acc: 0.9895 - val_loss: 0.0302 - val_acc: 0.9724\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0124 - acc: 0.9891 - val_loss: 0.0238 - val_acc: 0.9790\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist2/linear-8192-20230307-225444/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist2/linear-8192-20230307-225444/model\\assets\n"
     ]
    }
   ],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(image_w * image_h,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=8192, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/mnist2/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir+\"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=10, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=16, t=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "291/938 [========>.....................] - ETA: 8:21 - loss: 0.0622 - acc: 0.8941"
     ]
    }
   ],
   "source": [
    "model_hard_16_500 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/mnist2/hard-16x500-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_16_500.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_16_500.save(modeldir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_16_500 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/mnist2/soft-16x500-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_16_500.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_16_500.save(modeldir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=160, t=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hard_160_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/mnist2/hard-160x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_160_50.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_160_50.save(modeldir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_160_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/mnist2/soft-160x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_160_50.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_160_50.save(modeldir + \"/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

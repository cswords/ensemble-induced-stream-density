{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "!rm -rf ./logs/hypothyroid/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:  2640 n_classes:  2 dims:  29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "openml_dataset = fetch_openml(\n",
    "    name=\"hypothyroid\", version=2, as_frame=False, parser=\"liac-arff\"\n",
    ")\n",
    "\n",
    "n_total, dims = openml_dataset.data.shape\n",
    "n = int(n_total * 0.7)\n",
    "\n",
    "X = openml_dataset.data.astype(np.float32)\n",
    "y = preprocessing.LabelEncoder().fit_transform(openml_dataset.target)\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(n_total)\n",
    "\n",
    "ds_train_raw = ds_raw.take(n)\n",
    "ds_test_raw = ds_raw.skip(n)\n",
    "\n",
    "n_classes = np.unique(openml_dataset.target).shape[0]\n",
    "\n",
    "print(\"n: \", n, \"n_classes: \", n_classes, \"dims: \", dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_normalized = ds_train_raw.cache()\n",
    "\n",
    "ds_test_normalized = ds_test_raw.cache()\n",
    "\n",
    "\n",
    "def prepare(ds, batch_size=batch_size):\n",
    "    return ds.shuffle(n).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 2s 11ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 0s 8ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 0s 6ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 0s 6ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 0s 8ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 0s 8ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 0s 8ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 0s 7ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/linear-8192-20230310-222616/model\\assets\n"
     ]
    }
   ],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(dims,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=2000, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/hypothyroid/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir + \"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(\n",
    "            ds_train_raw.shuffle(n)\n",
    "            .take(psi)\n",
    "            .batch(psi)\n",
    "            .as_numpy_iterator()\n",
    "        )[0][0]\n",
    "        for _ in range(t)\n",
    "    ]\n",
    "\n",
    "\n",
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_ann_weighted(X, samples, sample_weights, p=2, soft=True):\n",
    "    m_dis = None  # [n, psi]\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]  # [i, dims]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )  # [n, 1]\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    m_dis = m_dis * sample_weights\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.sample_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann_weighted(inputs, self.samples, self.sample_weights, self.p)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 27s 334ms/step - loss: 0.3575 - acc: 0.8242 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.1717 - acc: 0.0000e+00 - val_loss: 0.1460 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1590 - acc: 0.2000 - val_loss: 0.1391 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 6s 150ms/step - loss: 0.1569 - acc: 0.2667 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: 0.1560 - acc: 0.5152 - val_loss: 0.1393 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: 0.1562 - acc: 0.6606 - val_loss: 0.1390 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: 0.1560 - acc: 0.6121 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1560 - acc: 0.5333 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1558 - acc: 0.5394 - val_loss: 0.1392 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.1558 - acc: 0.4667 - val_loss: 0.1392 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.1560 - acc: 0.5879 - val_loss: 0.1384 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: 0.1559 - acc: 0.3697 - val_loss: 0.1387 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1560 - acc: 0.6061 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: 0.1558 - acc: 0.6121 - val_loss: 0.1391 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: 0.1559 - acc: 0.6303 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: 0.1560 - acc: 0.3939 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1560 - acc: 0.3879 - val_loss: 0.1385 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.1570 - acc: 0.3152 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: 0.1582 - acc: 0.4182 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1564 - acc: 0.3152 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: 0.1560 - acc: 0.5333 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: 0.1559 - acc: 0.5091 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: 0.1561 - acc: 0.3394 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.1575 - acc: 0.3152 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: 0.1566 - acc: 0.7030 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1560 - acc: 0.4667 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: 0.1560 - acc: 0.4182 - val_loss: 0.1387 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1559 - acc: 0.5333 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1561 - acc: 0.4909 - val_loss: 0.1406 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1559 - acc: 0.5818 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/hard-20x50-20230310-222634/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "\n",
    "model_hard_20_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/hypothyroid/hard-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 31s 362ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 7s 167ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 7s 170ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 7s 160ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 7s 163ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 7s 163ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 7s 161ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 7s 164ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 7s 162ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 7s 166ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 7s 159ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 7s 166ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 7s 164ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 7s 164ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 7s 165ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 7s 171ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 7s 166ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 7s 169ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 7s 165ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/soft-20x50-20230310-223038/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_20_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/hypothyroid/soft-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 33s 396ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 8s 188ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 8s 186ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 8s 182ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 8s 185ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 8s 182ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 7s 178ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 7s 177ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 8s 181ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 7s 177ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 8s 181ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 7s 177ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 7s 178ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 8s 186ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 7s 176ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 7s 179ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 8s 182ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 7s 180ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 8s 185ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 8s 180ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 8s 180ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 8s 183ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 8s 188ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 8s 183ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 8s 184ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 7s 175ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 8s 181ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 8s 181ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 8s 186ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 8s 180ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/flex-20x50-20230310-223511/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/hypothyroid/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 25s 310ms/step - loss: 0.7367 - acc: 1.0000 - val_loss: 0.5239 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.3483 - acc: 1.0000 - val_loss: 0.1552 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.1605 - acc: 0.5091 - val_loss: 0.1426 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1579 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1550 - acc: 0.2909 - val_loss: 0.1381 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1549 - acc: 0.0970 - val_loss: 0.1380 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1547 - acc: 0.4606 - val_loss: 0.1381 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1547 - acc: 0.4182 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.1547 - acc: 0.4364 - val_loss: 0.1380 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1548 - acc: 0.4606 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 0.1549 - acc: 0.6121 - val_loss: 0.1382 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.1549 - acc: 0.6121 - val_loss: 0.1385 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1549 - acc: 0.5879 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: 0.1548 - acc: 0.5333 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.1548 - acc: 0.5576 - val_loss: 0.1382 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.1550 - acc: 0.2909 - val_loss: 0.1381 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 0.1548 - acc: 0.5818 - val_loss: 0.1381 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: 0.1548 - acc: 0.4848 - val_loss: 0.1380 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.1551 - acc: 0.8545 - val_loss: 0.1382 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: 0.1549 - acc: 0.2727 - val_loss: 0.1380 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.1549 - acc: 0.4667 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.1548 - acc: 0.3939 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1548 - acc: 0.4424 - val_loss: 0.1381 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1548 - acc: 0.5818 - val_loss: 0.1382 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 6s 136ms/step - loss: 0.1548 - acc: 0.4909 - val_loss: 0.1381 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.1550 - acc: 0.2667 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.1549 - acc: 0.2667 - val_loss: 0.1380 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.1548 - acc: 0.1455 - val_loss: 0.1381 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.1548 - acc: 0.5818 - val_loss: 0.1382 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 0.1548 - acc: 0.3394 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/hard-100x10-20230310-224009/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_hard_100_10 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/hypothyroid/hard-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 27s 329ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 6s 152ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 6s 152ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 6s 150ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 6s 150ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 6s 146ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 6s 147ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 6s 152ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 6s 151ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 6s 148ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 6s 151ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 6s 150ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 6s 150ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 6s 149ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/soft-100x10-20230310-224401/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_100_10 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/hypothyroid/soft-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 26s 310ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 6s 144ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 6s 143ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 6s 145ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 6s 139ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 6s 139ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 6s 136ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 6s 136ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 6s 135ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 6s 136ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 6s 140ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 6s 139ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 6s 137ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 5s 130ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 5s 129ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 6s 132ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 5s 129ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 5s 126ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 5s 130ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 5s 130ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 6s 136ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 5s 130ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 6s 141ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 5s 129ms/step - loss: nan - acc: 1.0000 - val_loss: nan - val_acc: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./logs/hypothyroid/flex-100x10-20230310-224804/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/hypothyroid/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "!rm -rf ./logs/hypothyroid/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenMLError",
     "evalue": "Dataset hyperthyroid with version 4 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenMLError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:303\u001b[0m, in \u001b[0;36m_get_data_info_by_name\u001b[1;34m(name, version, data_home, n_retries, delay)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     json_data \u001b[39m=\u001b[39m _get_json_content_from_openml_api(\n\u001b[0;32m    304\u001b[0m         url,\n\u001b[0;32m    305\u001b[0m         error_message\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    306\u001b[0m         data_home\u001b[39m=\u001b[39;49mdata_home,\n\u001b[0;32m    307\u001b[0m         n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[0;32m    308\u001b[0m         delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m OpenMLError:\n\u001b[0;32m    311\u001b[0m     \u001b[39m# we can do this in 1 function call if OpenML does not require the\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     \u001b[39m# specification of the dataset status (i.e., return datasets with a\u001b[39;00m\n\u001b[0;32m    313\u001b[0m     \u001b[39m# given name / version regardless of active, deactivated, etc. )\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39m# TODO: feature request OpenML.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:235\u001b[0m, in \u001b[0;36m_get_json_content_from_openml_api\u001b[1;34m(url, error_message, data_home, n_retries, delay)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m# 412 error, not in except for nicer traceback\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[39mraise\u001b[39;00m OpenMLError(error_message)\n",
      "\u001b[1;31mOpenMLError\u001b[0m: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpenMLError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_openml\n\u001b[1;32m----> 6\u001b[0m openml_dataset \u001b[39m=\u001b[39m fetch_openml(\n\u001b[0;32m      7\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhyperthyroid\u001b[39;49m\u001b[39m\"\u001b[39;49m, version\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, as_frame\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, parser\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mliac-arff\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m n_total, dims \u001b[39m=\u001b[39m openml_dataset\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape\n\u001b[0;32m     11\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_total \u001b[39m*\u001b[39m \u001b[39m0.7\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:882\u001b[0m, in \u001b[0;36mfetch_openml\u001b[1;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m data_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    878\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataset data_id=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and name=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m passed, but you can only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mspecify a numeric data_id or a name, not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mboth.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(data_id, name)\n\u001b[0;32m    881\u001b[0m         )\n\u001b[1;32m--> 882\u001b[0m     data_info \u001b[39m=\u001b[39m _get_data_info_by_name(\n\u001b[0;32m    883\u001b[0m         name, version, data_home, n_retries\u001b[39m=\u001b[39;49mn_retries, delay\u001b[39m=\u001b[39;49mdelay\n\u001b[0;32m    884\u001b[0m     )\n\u001b[0;32m    885\u001b[0m     data_id \u001b[39m=\u001b[39m data_info[\u001b[39m\"\u001b[39m\u001b[39mdid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    886\u001b[0m \u001b[39melif\u001b[39;00m data_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m     \u001b[39m# from the previous if statement, it is given that name is None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:317\u001b[0m, in \u001b[0;36m_get_data_info_by_name\u001b[1;34m(name, version, data_home, n_retries, delay)\u001b[0m\n\u001b[0;32m    315\u001b[0m     url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/status/deactivated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m     error_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with version \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, version)\n\u001b[1;32m--> 317\u001b[0m     json_data \u001b[39m=\u001b[39m _get_json_content_from_openml_api(\n\u001b[0;32m    318\u001b[0m         url,\n\u001b[0;32m    319\u001b[0m         error_msg,\n\u001b[0;32m    320\u001b[0m         data_home\u001b[39m=\u001b[39;49mdata_home,\n\u001b[0;32m    321\u001b[0m         n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[0;32m    322\u001b[0m         delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[0;32m    323\u001b[0m     )\n\u001b[0;32m    325\u001b[0m \u001b[39mreturn\u001b[39;00m json_data[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:235\u001b[0m, in \u001b[0;36m_get_json_content_from_openml_api\u001b[1;34m(url, error_message, data_home, n_retries, delay)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[39mraise\u001b[39;00m error\n\u001b[0;32m    234\u001b[0m \u001b[39m# 412 error, not in except for nicer traceback\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[39mraise\u001b[39;00m OpenMLError(error_message)\n",
      "\u001b[1;31mOpenMLError\u001b[0m: Dataset hyperthyroid with version 4 not found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "openml_dataset = fetch_openml(\n",
    "    name=\"hypothyroid\", version=4, as_frame=False, parser=\"liac-arff\"\n",
    ")\n",
    "\n",
    "n_total, dims = openml_dataset.data.shape\n",
    "n = int(n_total * 0.7)\n",
    "\n",
    "X = openml_dataset.data.astype(np.float32)\n",
    "y = preprocessing.LabelEncoder().fit_transform(openml_dataset.target)\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(n_total)\n",
    "\n",
    "ds_train_raw = ds_raw.take(n)\n",
    "ds_test_raw = ds_raw.skip(n)\n",
    "\n",
    "n_classes = np.unique(openml_dataset.target).shape[0]\n",
    "\n",
    "print(\"n: \", n, \"n_classes: \", n_classes, \"dims: \", dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_normalized = ds_train_raw.cache()\n",
    "\n",
    "ds_test_normalized = ds_test_raw.cache()\n",
    "\n",
    "\n",
    "def prepare(ds, batch_size=batch_size):\n",
    "    return ds.shuffle(n).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(dims,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=2000, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/hypothyroid/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir + \"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(\n",
    "            ds_train_raw.shuffle(n)\n",
    "            .take(psi)\n",
    "            .batch(psi)\n",
    "            .as_numpy_iterator()\n",
    "        )[0][0]\n",
    "        for _ in range(t)\n",
    "    ]\n",
    "\n",
    "\n",
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_ann_weighted(X, samples, sample_weights, p=2, soft=True):\n",
    "    m_dis = None  # [n, psi]\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]  # [i, dims]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )  # [n, 1]\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    m_dis = m_dis * sample_weights\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.sample_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann_weighted(inputs, self.samples, self.sample_weights, self.p)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "\n",
    "model_hard_20_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/hypothyroid/hard-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_20_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/hypothyroid/soft-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/hypothyroid/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_hard_100_10 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/hypothyroid/hard-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_100_10 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/hypothyroid/soft-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/hypothyroid/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

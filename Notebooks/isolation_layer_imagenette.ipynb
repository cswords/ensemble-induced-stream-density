{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "image_w, image_h = 32, 32\n",
    "\n",
    "!rm -rf ./logs/imagenette/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:  9469 n_classes:  10 dims:  3072\n"
     ]
    }
   ],
   "source": [
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    \"imagenette\",\n",
    "    split=[\"train\", \"validation\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "n_classes = ds_info.features[\"label\"].num_classes\n",
    "n = ds_info.splits[\"train\"].num_examples\n",
    "\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = layers.Resizing(image_h, image_w)(image)\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(tf.cast(label, tf.int32), n_classes)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def normalize(ds):\n",
    "    ds = (\n",
    "        ds.cache()\n",
    "        .map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "_, dims = list(\n",
    "    ds_train_raw.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .take(1)\n",
    "    .batch(1)\n",
    "    .as_numpy_iterator()\n",
    ")[0][0].shape\n",
    "\n",
    "print(\"n: \", n, \"n_classes: \", n_classes, \"dims: \", dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 9s 50ms/step - loss: 0.3514 - acc: 0.2651 - val_loss: 0.3049 - val_acc: 0.3195\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 6s 40ms/step - loss: 0.2606 - acc: 0.4049 - val_loss: 0.2859 - val_acc: 0.3557\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.2257 - acc: 0.4969 - val_loss: 0.2822 - val_acc: 0.3684\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 6s 43ms/step - loss: 0.2058 - acc: 0.5461 - val_loss: 0.2824 - val_acc: 0.3699\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 7s 45ms/step - loss: 0.1928 - acc: 0.5824 - val_loss: 0.2856 - val_acc: 0.3674\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 6s 44ms/step - loss: 0.1835 - acc: 0.6039 - val_loss: 0.2904 - val_acc: 0.3692\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1764 - acc: 0.6282 - val_loss: 0.2946 - val_acc: 0.3656\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1706 - acc: 0.6430 - val_loss: 0.3001 - val_acc: 0.3676\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 6s 43ms/step - loss: 0.1662 - acc: 0.6592 - val_loss: 0.3055 - val_acc: 0.3659\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 6s 43ms/step - loss: 0.1625 - acc: 0.6684 - val_loss: 0.3105 - val_acc: 0.3613\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1595 - acc: 0.6761 - val_loss: 0.3157 - val_acc: 0.3615\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1566 - acc: 0.6841 - val_loss: 0.3217 - val_acc: 0.3564\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1540 - acc: 0.6965 - val_loss: 0.3287 - val_acc: 0.3541\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1517 - acc: 0.7024 - val_loss: 0.3351 - val_acc: 0.3518\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1492 - acc: 0.7105 - val_loss: 0.3382 - val_acc: 0.3411\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1474 - acc: 0.7149 - val_loss: 0.3413 - val_acc: 0.3376\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1457 - acc: 0.7184 - val_loss: 0.3449 - val_acc: 0.3411\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1448 - acc: 0.7202 - val_loss: 0.3471 - val_acc: 0.3460\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1439 - acc: 0.7221 - val_loss: 0.3533 - val_acc: 0.3383\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1421 - acc: 0.7268 - val_loss: 0.3586 - val_acc: 0.3417\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1410 - acc: 0.7319 - val_loss: 0.3659 - val_acc: 0.3439\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1398 - acc: 0.7351 - val_loss: 0.3661 - val_acc: 0.3462\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1382 - acc: 0.7395 - val_loss: 0.3695 - val_acc: 0.3411\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1372 - acc: 0.7432 - val_loss: 0.3716 - val_acc: 0.3358\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1362 - acc: 0.7424 - val_loss: 0.3767 - val_acc: 0.3366\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1349 - acc: 0.7472 - val_loss: 0.3808 - val_acc: 0.3320\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1327 - acc: 0.7508 - val_loss: 0.3870 - val_acc: 0.3274\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1319 - acc: 0.7568 - val_loss: 0.3914 - val_acc: 0.3338\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 6s 42ms/step - loss: 0.1305 - acc: 0.7587 - val_loss: 0.3991 - val_acc: 0.3332\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.1300 - acc: 0.7617 - val_loss: 0.4025 - val_acc: 0.3353\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/linear-8192-20230308-114934/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/linear-8192-20230308-114934/model\\assets\n"
     ]
    }
   ],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(dims,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=2000, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/imagenette/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir + \"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(\n",
    "            ds_train_raw.shuffle(n)\n",
    "            .take(psi)\n",
    "            .map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(psi)\n",
    "            .as_numpy_iterator()\n",
    "        )[0][0]\n",
    "        for _ in range(t)\n",
    "    ]\n",
    "\n",
    "\n",
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.dimential_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p) * self.dimential_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 49s 237ms/step - loss: 0.3014 - acc: 0.2111 - val_loss: 0.2202 - val_acc: 0.2591\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 26s 177ms/step - loss: 0.2127 - acc: 0.2990 - val_loss: 0.2124 - val_acc: 0.2803\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.2057 - acc: 0.3433 - val_loss: 0.2088 - val_acc: 0.2991\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.2014 - acc: 0.3733 - val_loss: 0.2064 - val_acc: 0.3141\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 26s 176ms/step - loss: 0.1986 - acc: 0.4006 - val_loss: 0.2052 - val_acc: 0.3162\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 26s 176ms/step - loss: 0.1967 - acc: 0.4152 - val_loss: 0.2042 - val_acc: 0.3213\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 26s 175ms/step - loss: 0.1954 - acc: 0.4257 - val_loss: 0.2037 - val_acc: 0.3185\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 26s 175ms/step - loss: 0.1944 - acc: 0.4355 - val_loss: 0.2035 - val_acc: 0.3192\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 26s 175ms/step - loss: 0.1936 - acc: 0.4395 - val_loss: 0.2033 - val_acc: 0.3236\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1930 - acc: 0.4449 - val_loss: 0.2033 - val_acc: 0.3177\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1925 - acc: 0.4478 - val_loss: 0.2033 - val_acc: 0.3200\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1921 - acc: 0.4554 - val_loss: 0.2036 - val_acc: 0.3241\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1917 - acc: 0.4561 - val_loss: 0.2036 - val_acc: 0.3228\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1914 - acc: 0.4608 - val_loss: 0.2039 - val_acc: 0.3231\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1911 - acc: 0.4600 - val_loss: 0.2042 - val_acc: 0.3233\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 26s 175ms/step - loss: 0.1909 - acc: 0.4619 - val_loss: 0.2046 - val_acc: 0.3215\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1906 - acc: 0.4646 - val_loss: 0.2050 - val_acc: 0.3220\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1904 - acc: 0.4656 - val_loss: 0.2053 - val_acc: 0.3238\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 26s 176ms/step - loss: 0.1902 - acc: 0.4698 - val_loss: 0.2056 - val_acc: 0.3231\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1900 - acc: 0.4682 - val_loss: 0.2059 - val_acc: 0.3220\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1898 - acc: 0.4704 - val_loss: 0.2064 - val_acc: 0.3236\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1896 - acc: 0.4720 - val_loss: 0.2066 - val_acc: 0.3182\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1894 - acc: 0.4749 - val_loss: 0.2069 - val_acc: 0.3243\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1892 - acc: 0.4794 - val_loss: 0.2073 - val_acc: 0.3210\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1891 - acc: 0.4779 - val_loss: 0.2076 - val_acc: 0.3164\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1889 - acc: 0.4786 - val_loss: 0.2080 - val_acc: 0.3223\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 26s 174ms/step - loss: 0.1888 - acc: 0.4786 - val_loss: 0.2083 - val_acc: 0.3213\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1886 - acc: 0.4819 - val_loss: 0.2088 - val_acc: 0.3205\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1885 - acc: 0.4797 - val_loss: 0.2091 - val_acc: 0.3177\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 26s 173ms/step - loss: 0.1883 - acc: 0.4813 - val_loss: 0.2094 - val_acc: 0.3172\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/hard-20x50-20230308-115629/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/hard-20x50-20230308-115629/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "\n",
    "model_hard_20_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/imagenette/hard-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_20_50.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 56s 275ms/step - loss: 0.6282 - acc: 0.2211 - val_loss: 0.5057 - val_acc: 0.2576\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.4534 - acc: 0.2761 - val_loss: 0.4300 - val_acc: 0.2836\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 27s 182ms/step - loss: 0.3924 - acc: 0.3003 - val_loss: 0.3803 - val_acc: 0.2996\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 26s 178ms/step - loss: 0.3477 - acc: 0.3209 - val_loss: 0.3397 - val_acc: 0.3118\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 27s 182ms/step - loss: 0.3105 - acc: 0.3390 - val_loss: 0.3048 - val_acc: 0.3340\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 27s 182ms/step - loss: 0.2792 - acc: 0.3596 - val_loss: 0.2756 - val_acc: 0.3366\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 27s 179ms/step - loss: 0.2541 - acc: 0.3736 - val_loss: 0.2525 - val_acc: 0.3409\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 27s 179ms/step - loss: 0.2352 - acc: 0.3853 - val_loss: 0.2350 - val_acc: 0.3526\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.2221 - acc: 0.3930 - val_loss: 0.2238 - val_acc: 0.3585\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.2137 - acc: 0.4055 - val_loss: 0.2168 - val_acc: 0.3618\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 27s 182ms/step - loss: 0.2082 - acc: 0.4181 - val_loss: 0.2122 - val_acc: 0.3610\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.2044 - acc: 0.4230 - val_loss: 0.2089 - val_acc: 0.3674\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.2017 - acc: 0.4286 - val_loss: 0.2065 - val_acc: 0.3684\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.1995 - acc: 0.4342 - val_loss: 0.2046 - val_acc: 0.3715\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 26s 178ms/step - loss: 0.1978 - acc: 0.4375 - val_loss: 0.2031 - val_acc: 0.3738\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1964 - acc: 0.4409 - val_loss: 0.2019 - val_acc: 0.3730\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1953 - acc: 0.4448 - val_loss: 0.2010 - val_acc: 0.3730\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1945 - acc: 0.4474 - val_loss: 0.2002 - val_acc: 0.3710\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.1937 - acc: 0.4491 - val_loss: 0.1996 - val_acc: 0.3755\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1931 - acc: 0.4532 - val_loss: 0.1992 - val_acc: 0.3732\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 27s 179ms/step - loss: 0.1926 - acc: 0.4558 - val_loss: 0.1988 - val_acc: 0.3743\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1922 - acc: 0.4574 - val_loss: 0.1986 - val_acc: 0.3748\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1918 - acc: 0.4600 - val_loss: 0.1983 - val_acc: 0.3768\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.1915 - acc: 0.4615 - val_loss: 0.1981 - val_acc: 0.3748\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1912 - acc: 0.4632 - val_loss: 0.1979 - val_acc: 0.3722\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 27s 179ms/step - loss: 0.1910 - acc: 0.4632 - val_loss: 0.1978 - val_acc: 0.3748\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 27s 181ms/step - loss: 0.1907 - acc: 0.4653 - val_loss: 0.1977 - val_acc: 0.3710\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.1905 - acc: 0.4674 - val_loss: 0.1977 - val_acc: 0.3743\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 26s 179ms/step - loss: 0.1903 - acc: 0.4673 - val_loss: 0.1975 - val_acc: 0.3761\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 27s 180ms/step - loss: 0.1901 - acc: 0.4685 - val_loss: 0.1974 - val_acc: 0.3743\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/soft-20x50-20230308-121050/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/soft-20x50-20230308-121050/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_20_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/imagenette/soft-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_20_50.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 61s 280ms/step - loss: 0.6235 - acc: 0.2239 - val_loss: 0.4984 - val_acc: 0.2632\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 30s 205ms/step - loss: 0.4443 - acc: 0.2846 - val_loss: 0.4234 - val_acc: 0.2833\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 30s 206ms/step - loss: 0.3848 - acc: 0.3100 - val_loss: 0.3746 - val_acc: 0.2983\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 30s 206ms/step - loss: 0.3412 - acc: 0.3287 - val_loss: 0.3346 - val_acc: 0.3149\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 30s 206ms/step - loss: 0.3051 - acc: 0.3504 - val_loss: 0.3012 - val_acc: 0.3289\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 31s 207ms/step - loss: 0.2756 - acc: 0.3720 - val_loss: 0.2737 - val_acc: 0.3338\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 31s 210ms/step - loss: 0.2528 - acc: 0.3783 - val_loss: 0.2524 - val_acc: 0.3442\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 30s 201ms/step - loss: 0.2359 - acc: 0.3919 - val_loss: 0.2369 - val_acc: 0.3539\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.2239 - acc: 0.4035 - val_loss: 0.2258 - val_acc: 0.3572\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.2152 - acc: 0.4132 - val_loss: 0.2180 - val_acc: 0.3628\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 30s 199ms/step - loss: 0.2090 - acc: 0.4197 - val_loss: 0.2126 - val_acc: 0.3697\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.2046 - acc: 0.4272 - val_loss: 0.2089 - val_acc: 0.3689\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 33s 224ms/step - loss: 0.2015 - acc: 0.4324 - val_loss: 0.2061 - val_acc: 0.3704\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 31s 207ms/step - loss: 0.1991 - acc: 0.4395 - val_loss: 0.2042 - val_acc: 0.3738\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.1973 - acc: 0.4429 - val_loss: 0.2029 - val_acc: 0.3743\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.1959 - acc: 0.4439 - val_loss: 0.2018 - val_acc: 0.3699\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.1947 - acc: 0.4472 - val_loss: 0.2008 - val_acc: 0.3753\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.1938 - acc: 0.4498 - val_loss: 0.2002 - val_acc: 0.3738\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 35s 238ms/step - loss: 0.1931 - acc: 0.4521 - val_loss: 0.1996 - val_acc: 0.3725\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1925 - acc: 0.4560 - val_loss: 0.1993 - val_acc: 0.3722\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.1920 - acc: 0.4572 - val_loss: 0.1989 - val_acc: 0.3755\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1915 - acc: 0.4597 - val_loss: 0.1986 - val_acc: 0.3732\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1911 - acc: 0.4609 - val_loss: 0.1985 - val_acc: 0.3786\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 30s 199ms/step - loss: 0.1908 - acc: 0.4611 - val_loss: 0.1983 - val_acc: 0.3763\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 30s 200ms/step - loss: 0.1904 - acc: 0.4633 - val_loss: 0.1982 - val_acc: 0.3753\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1902 - acc: 0.4669 - val_loss: 0.1981 - val_acc: 0.3745\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 30s 199ms/step - loss: 0.1899 - acc: 0.4676 - val_loss: 0.1980 - val_acc: 0.3732\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1897 - acc: 0.4690 - val_loss: 0.1979 - val_acc: 0.3722\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 29s 199ms/step - loss: 0.1895 - acc: 0.4696 - val_loss: 0.1979 - val_acc: 0.3773\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 30s 199ms/step - loss: 0.1893 - acc: 0.4721 - val_loss: 0.1977 - val_acc: 0.3758\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/flex-20x50-20230308-122548/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/flex-20x50-20230308-122548/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/imagenette/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 50s 253ms/step - loss: 0.5182 - acc: 0.1743 - val_loss: 0.2268 - val_acc: 0.2321\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.2147 - acc: 0.2774 - val_loss: 0.2109 - val_acc: 0.2854\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 27s 182ms/step - loss: 0.2054 - acc: 0.3289 - val_loss: 0.2056 - val_acc: 0.3029\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.2007 - acc: 0.3561 - val_loss: 0.2028 - val_acc: 0.3118\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1979 - acc: 0.3752 - val_loss: 0.2009 - val_acc: 0.3177\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1960 - acc: 0.3865 - val_loss: 0.1997 - val_acc: 0.3175\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1947 - acc: 0.3960 - val_loss: 0.1989 - val_acc: 0.3172\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1937 - acc: 0.3995 - val_loss: 0.1982 - val_acc: 0.3159\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1929 - acc: 0.4041 - val_loss: 0.1978 - val_acc: 0.3141\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1924 - acc: 0.4047 - val_loss: 0.1974 - val_acc: 0.3101\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 28s 192ms/step - loss: 0.1919 - acc: 0.4010 - val_loss: 0.1972 - val_acc: 0.3075\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1915 - acc: 0.4008 - val_loss: 0.1970 - val_acc: 0.3039\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1911 - acc: 0.3991 - val_loss: 0.1968 - val_acc: 0.3004\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 27s 183ms/step - loss: 0.1908 - acc: 0.4037 - val_loss: 0.1967 - val_acc: 0.3022\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1906 - acc: 0.3980 - val_loss: 0.1966 - val_acc: 0.3022\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1903 - acc: 0.3990 - val_loss: 0.1966 - val_acc: 0.3004\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 27s 184ms/step - loss: 0.1901 - acc: 0.3960 - val_loss: 0.1966 - val_acc: 0.2961\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.1899 - acc: 0.3984 - val_loss: 0.1965 - val_acc: 0.2940\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 29s 193ms/step - loss: 0.1897 - acc: 0.3987 - val_loss: 0.1965 - val_acc: 0.2958\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 29s 197ms/step - loss: 0.1895 - acc: 0.3991 - val_loss: 0.1965 - val_acc: 0.3014\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 28s 192ms/step - loss: 0.1894 - acc: 0.4016 - val_loss: 0.1965 - val_acc: 0.2932\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 28s 190ms/step - loss: 0.1892 - acc: 0.4007 - val_loss: 0.1964 - val_acc: 0.3001\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 28s 192ms/step - loss: 0.1891 - acc: 0.3999 - val_loss: 0.1965 - val_acc: 0.2950\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 28s 193ms/step - loss: 0.1889 - acc: 0.3996 - val_loss: 0.1965 - val_acc: 0.2966\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 29s 194ms/step - loss: 0.1888 - acc: 0.3952 - val_loss: 0.1965 - val_acc: 0.2973\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 29s 197ms/step - loss: 0.1886 - acc: 0.3998 - val_loss: 0.1966 - val_acc: 0.2961\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 29s 196ms/step - loss: 0.1885 - acc: 0.4026 - val_loss: 0.1966 - val_acc: 0.2950\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 28s 193ms/step - loss: 0.1884 - acc: 0.3995 - val_loss: 0.1966 - val_acc: 0.2940\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 28s 192ms/step - loss: 0.1882 - acc: 0.4014 - val_loss: 0.1967 - val_acc: 0.2983\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 29s 193ms/step - loss: 0.1881 - acc: 0.4028 - val_loss: 0.1966 - val_acc: 0.2961\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/hard-100x10-20230308-124442/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/hard-100x10-20230308-124442/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_hard_100_10 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/imagenette/hard-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_100_10.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 56s 285ms/step - loss: 0.6286 - acc: 0.2021 - val_loss: 0.5071 - val_acc: 0.2504\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 29s 198ms/step - loss: 0.4541 - acc: 0.2769 - val_loss: 0.4322 - val_acc: 0.2790\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 31s 209ms/step - loss: 0.3938 - acc: 0.3017 - val_loss: 0.3821 - val_acc: 0.3001\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 31s 207ms/step - loss: 0.3490 - acc: 0.3223 - val_loss: 0.3408 - val_acc: 0.3149\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.3115 - acc: 0.3445 - val_loss: 0.3054 - val_acc: 0.3276\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.2804 - acc: 0.3636 - val_loss: 0.2763 - val_acc: 0.3371\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.2556 - acc: 0.3758 - val_loss: 0.2531 - val_acc: 0.3427\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.2364 - acc: 0.3877 - val_loss: 0.2353 - val_acc: 0.3475\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.2229 - acc: 0.3968 - val_loss: 0.2236 - val_acc: 0.3541\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.2141 - acc: 0.4082 - val_loss: 0.2159 - val_acc: 0.3615\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 30s 203ms/step - loss: 0.2083 - acc: 0.4181 - val_loss: 0.2112 - val_acc: 0.3692\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.2045 - acc: 0.4243 - val_loss: 0.2080 - val_acc: 0.3669\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 30s 206ms/step - loss: 0.2017 - acc: 0.4304 - val_loss: 0.2059 - val_acc: 0.3697\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1996 - acc: 0.4335 - val_loss: 0.2042 - val_acc: 0.3748\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1980 - acc: 0.4408 - val_loss: 0.2027 - val_acc: 0.3773\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 31s 207ms/step - loss: 0.1966 - acc: 0.4427 - val_loss: 0.2017 - val_acc: 0.3781\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 30s 203ms/step - loss: 0.1955 - acc: 0.4475 - val_loss: 0.2007 - val_acc: 0.3750\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1947 - acc: 0.4490 - val_loss: 0.2000 - val_acc: 0.3725\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1939 - acc: 0.4517 - val_loss: 0.1995 - val_acc: 0.3738\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 30s 206ms/step - loss: 0.1934 - acc: 0.4545 - val_loss: 0.1991 - val_acc: 0.3735\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1929 - acc: 0.4559 - val_loss: 0.1988 - val_acc: 0.3748\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 31s 210ms/step - loss: 0.1925 - acc: 0.4617 - val_loss: 0.1986 - val_acc: 0.3740\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 31s 208ms/step - loss: 0.1921 - acc: 0.4631 - val_loss: 0.1984 - val_acc: 0.3712\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 30s 203ms/step - loss: 0.1918 - acc: 0.4640 - val_loss: 0.1981 - val_acc: 0.3720\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 31s 207ms/step - loss: 0.1915 - acc: 0.4681 - val_loss: 0.1982 - val_acc: 0.3735\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 30s 202ms/step - loss: 0.1913 - acc: 0.4698 - val_loss: 0.1979 - val_acc: 0.3755\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 31s 208ms/step - loss: 0.1910 - acc: 0.4696 - val_loss: 0.1980 - val_acc: 0.3715\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 30s 204ms/step - loss: 0.1908 - acc: 0.4719 - val_loss: 0.1980 - val_acc: 0.3720\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 30s 201ms/step - loss: 0.1906 - acc: 0.4730 - val_loss: 0.1978 - val_acc: 0.3707\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 30s 201ms/step - loss: 0.1905 - acc: 0.4762 - val_loss: 0.1979 - val_acc: 0.3684\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/soft-100x10-20230308-130005/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/soft-100x10-20230308-130005/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_100_10 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/imagenette/soft-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_100_10.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 61s 313ms/step - loss: 0.6250 - acc: 0.2046 - val_loss: 0.4967 - val_acc: 0.2609\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 31s 210ms/step - loss: 0.4451 - acc: 0.2782 - val_loss: 0.4234 - val_acc: 0.2831\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.3863 - acc: 0.3038 - val_loss: 0.3759 - val_acc: 0.3027\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 32s 216ms/step - loss: 0.3429 - acc: 0.3288 - val_loss: 0.3362 - val_acc: 0.3220\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 32s 215ms/step - loss: 0.3069 - acc: 0.3513 - val_loss: 0.3026 - val_acc: 0.3350\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.2774 - acc: 0.3681 - val_loss: 0.2751 - val_acc: 0.3460\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.2543 - acc: 0.3857 - val_loss: 0.2532 - val_acc: 0.3506\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.2368 - acc: 0.3930 - val_loss: 0.2373 - val_acc: 0.3618\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 31s 213ms/step - loss: 0.2243 - acc: 0.4060 - val_loss: 0.2258 - val_acc: 0.3659\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 32s 216ms/step - loss: 0.2154 - acc: 0.4184 - val_loss: 0.2178 - val_acc: 0.3697\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.2091 - acc: 0.4251 - val_loss: 0.2122 - val_acc: 0.3710\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.2047 - acc: 0.4310 - val_loss: 0.2084 - val_acc: 0.3740\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 32s 215ms/step - loss: 0.2016 - acc: 0.4340 - val_loss: 0.2057 - val_acc: 0.3817\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 31s 209ms/step - loss: 0.1992 - acc: 0.4373 - val_loss: 0.2038 - val_acc: 0.3827\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1973 - acc: 0.4418 - val_loss: 0.2024 - val_acc: 0.3814\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1959 - acc: 0.4461 - val_loss: 0.2013 - val_acc: 0.3814\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 31s 212ms/step - loss: 0.1948 - acc: 0.4479 - val_loss: 0.2004 - val_acc: 0.3834\n",
      "Epoch 18/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1939 - acc: 0.4495 - val_loss: 0.1997 - val_acc: 0.3855\n",
      "Epoch 19/30\n",
      "148/148 [==============================] - 31s 209ms/step - loss: 0.1931 - acc: 0.4544 - val_loss: 0.1992 - val_acc: 0.3845\n",
      "Epoch 20/30\n",
      "148/148 [==============================] - 31s 210ms/step - loss: 0.1925 - acc: 0.4569 - val_loss: 0.1987 - val_acc: 0.3842\n",
      "Epoch 21/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1920 - acc: 0.4561 - val_loss: 0.1984 - val_acc: 0.3845\n",
      "Epoch 22/30\n",
      "148/148 [==============================] - 31s 212ms/step - loss: 0.1915 - acc: 0.4611 - val_loss: 0.1981 - val_acc: 0.3827\n",
      "Epoch 23/30\n",
      "148/148 [==============================] - 32s 213ms/step - loss: 0.1911 - acc: 0.4616 - val_loss: 0.1980 - val_acc: 0.3822\n",
      "Epoch 24/30\n",
      "148/148 [==============================] - 32s 213ms/step - loss: 0.1907 - acc: 0.4650 - val_loss: 0.1978 - val_acc: 0.3847\n",
      "Epoch 25/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1905 - acc: 0.4651 - val_loss: 0.1976 - val_acc: 0.3834\n",
      "Epoch 26/30\n",
      "148/148 [==============================] - 31s 213ms/step - loss: 0.1902 - acc: 0.4679 - val_loss: 0.1975 - val_acc: 0.3801\n",
      "Epoch 27/30\n",
      "148/148 [==============================] - 32s 215ms/step - loss: 0.1899 - acc: 0.4709 - val_loss: 0.1974 - val_acc: 0.3814\n",
      "Epoch 28/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.1897 - acc: 0.4720 - val_loss: 0.1974 - val_acc: 0.3806\n",
      "Epoch 29/30\n",
      "148/148 [==============================] - 32s 214ms/step - loss: 0.1894 - acc: 0.4731 - val_loss: 0.1973 - val_acc: 0.3814\n",
      "Epoch 30/30\n",
      "148/148 [==============================] - 31s 211ms/step - loss: 0.1892 - acc: 0.4740 - val_loss: 0.1972 - val_acc: 0.3806\n",
      "INFO:tensorflow:Assets written to: ./logs/imagenette/flex-100x10-20230308-131647/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/imagenette/flex-100x10-20230308-131647/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/imagenette/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    normalize(ds_train_raw),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=normalize(ds_test_raw),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

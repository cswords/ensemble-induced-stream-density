{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "image_w, image_h = 32, 32\n",
    "\n",
    "!rm -rf ./logs/mnist/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:  60000 n_classes:  10 dims:  1024\n"
     ]
    }
   ],
   "source": [
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "n_classes = ds_info.features[\"label\"].num_classes\n",
    "n = ds_info.splits[\"train\"].num_examples\n",
    "\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = layers.Resizing(image_h, image_w)(image)\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(tf.cast(label, tf.int32), n_classes)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "ds_train_normalized = ds_train_raw.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache()\n",
    "\n",
    "ds_test_normalized = ds_test_raw.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache()\n",
    "\n",
    "\n",
    "def prepare(ds, batch_size=batch_size):\n",
    "    return ds.shuffle(n).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dims = list(ds_train_normalized.take(1))[0][0].shape[0]\n",
    "\n",
    "print(\"n: \", n, \"n_classes: \", n_classes, \"dims: \", dims)\n",
    "\n",
    "\n",
    "def minmax_reducer(current, input):\n",
    "    X, _ = input\n",
    "    return (\n",
    "        tf.reduce_min([current[0], X], axis=0),\n",
    "        tf.reduce_max([current[0], X], axis=0),\n",
    "    )\n",
    "\n",
    "\n",
    "x0, _ = list(ds_train_normalized.take(1))[0]\n",
    "min_train, max_train = ds_train_normalized.reduce((x0, x0), minmax_reducer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 0.0661 - acc: 0.8933 - val_loss: 0.0414 - val_acc: 0.9371\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0354 - acc: 0.9478 - val_loss: 0.0290 - val_acc: 0.9573\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0293 - acc: 0.9572 - val_loss: 0.0289 - val_acc: 0.9560\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0257 - acc: 0.9637 - val_loss: 0.0267 - val_acc: 0.9596\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0230 - acc: 0.9675 - val_loss: 0.0269 - val_acc: 0.9630\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0219 - acc: 0.9696 - val_loss: 0.0218 - val_acc: 0.9675\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0206 - acc: 0.9710 - val_loss: 0.0263 - val_acc: 0.9664\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0195 - acc: 0.9736 - val_loss: 0.0198 - val_acc: 0.9701\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0181 - acc: 0.9757 - val_loss: 0.0205 - val_acc: 0.9700\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0174 - acc: 0.9772 - val_loss: 0.0194 - val_acc: 0.9730\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0166 - acc: 0.9782 - val_loss: 0.0205 - val_acc: 0.9702\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0162 - acc: 0.9796 - val_loss: 0.0209 - val_acc: 0.9706\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0155 - acc: 0.9794 - val_loss: 0.0200 - val_acc: 0.9712\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0150 - acc: 0.9813 - val_loss: 0.0208 - val_acc: 0.9710\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0146 - acc: 0.9818 - val_loss: 0.0198 - val_acc: 0.9722\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0143 - acc: 0.9826 - val_loss: 0.0211 - val_acc: 0.9715\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0139 - acc: 0.9823 - val_loss: 0.0210 - val_acc: 0.9704\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0135 - acc: 0.9834 - val_loss: 0.0200 - val_acc: 0.9715\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0129 - acc: 0.9842 - val_loss: 0.0184 - val_acc: 0.9744\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0127 - acc: 0.9845 - val_loss: 0.0201 - val_acc: 0.9725\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0123 - acc: 0.9850 - val_loss: 0.0199 - val_acc: 0.9717\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0124 - acc: 0.9852 - val_loss: 0.0196 - val_acc: 0.9732\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0116 - acc: 0.9864 - val_loss: 0.0194 - val_acc: 0.9721\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0118 - acc: 0.9862 - val_loss: 0.0196 - val_acc: 0.9725\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0115 - acc: 0.9869 - val_loss: 0.0187 - val_acc: 0.9736\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0115 - acc: 0.9866 - val_loss: 0.0194 - val_acc: 0.9728\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0110 - acc: 0.9874 - val_loss: 0.0196 - val_acc: 0.9733\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0109 - acc: 0.9876 - val_loss: 0.0187 - val_acc: 0.9746\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0110 - acc: 0.9872 - val_loss: 0.0217 - val_acc: 0.9704\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0104 - acc: 0.9879 - val_loss: 0.0200 - val_acc: 0.9738\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/linear-8192-20230310-102730/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/linear-8192-20230310-102730/model\\assets\n"
     ]
    }
   ],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(dims,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=2000, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/mnist/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir + \"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(\n",
    "            ds_train_raw.shuffle(n)\n",
    "            .take(psi)\n",
    "            .map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(psi)\n",
    "            .as_numpy_iterator()\n",
    "        )[0][0]\n",
    "        for _ in range(t)\n",
    "    ]\n",
    "\n",
    "\n",
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_ann_weighted(X, samples, sample_weights, p=2, soft=True):\n",
    "    m_dis = None  # [n, psi]\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]  # [i, dims]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )  # [n, 1]\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    m_dis = m_dis * sample_weights\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.sample_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann_weighted(inputs, self.samples, self.sample_weights, self.p)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 116s 111ms/step - loss: 0.0871 - acc: 0.8589 - val_loss: 0.0481 - val_acc: 0.9199\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 96s 102ms/step - loss: 0.0456 - acc: 0.9220 - val_loss: 0.0407 - val_acc: 0.9312\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 99s 105ms/step - loss: 0.0398 - acc: 0.9326 - val_loss: 0.0378 - val_acc: 0.9343\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0368 - acc: 0.9383 - val_loss: 0.0364 - val_acc: 0.9371\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0349 - acc: 0.9417 - val_loss: 0.0356 - val_acc: 0.9404\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0335 - acc: 0.9447 - val_loss: 0.0352 - val_acc: 0.9398\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0325 - acc: 0.9462 - val_loss: 0.0350 - val_acc: 0.9418\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0318 - acc: 0.9475 - val_loss: 0.0348 - val_acc: 0.9409\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 98s 105ms/step - loss: 0.0311 - acc: 0.9489 - val_loss: 0.0346 - val_acc: 0.9415\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0305 - acc: 0.9499 - val_loss: 0.0347 - val_acc: 0.9412\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0301 - acc: 0.9505 - val_loss: 0.0347 - val_acc: 0.9414\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 99s 105ms/step - loss: 0.0298 - acc: 0.9515 - val_loss: 0.0348 - val_acc: 0.9416\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0294 - acc: 0.9521 - val_loss: 0.0348 - val_acc: 0.9409\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0291 - acc: 0.9528 - val_loss: 0.0351 - val_acc: 0.9417\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0288 - acc: 0.9532 - val_loss: 0.0350 - val_acc: 0.9398\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0286 - acc: 0.9535 - val_loss: 0.0351 - val_acc: 0.9399\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0284 - acc: 0.9538 - val_loss: 0.0349 - val_acc: 0.9412\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0282 - acc: 0.9543 - val_loss: 0.0352 - val_acc: 0.9410\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0281 - acc: 0.9550 - val_loss: 0.0352 - val_acc: 0.9408\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0279 - acc: 0.9552 - val_loss: 0.0354 - val_acc: 0.9396\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0277 - acc: 0.9556 - val_loss: 0.0353 - val_acc: 0.9422\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0276 - acc: 0.9562 - val_loss: 0.0354 - val_acc: 0.9419\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0275 - acc: 0.9556 - val_loss: 0.0354 - val_acc: 0.9426\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 97s 103ms/step - loss: 0.0274 - acc: 0.9560 - val_loss: 0.0355 - val_acc: 0.9414\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0272 - acc: 0.9563 - val_loss: 0.0357 - val_acc: 0.9407\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0271 - acc: 0.9565 - val_loss: 0.0357 - val_acc: 0.9416\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0270 - acc: 0.9569 - val_loss: 0.0361 - val_acc: 0.9409\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0270 - acc: 0.9572 - val_loss: 0.0360 - val_acc: 0.9403\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 98s 104ms/step - loss: 0.0268 - acc: 0.9572 - val_loss: 0.0360 - val_acc: 0.9407\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 97s 104ms/step - loss: 0.0268 - acc: 0.9574 - val_loss: 0.0362 - val_acc: 0.9412\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/hard-20x50-20230310-102955/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/hard-20x50-20230310-102955/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "\n",
    "model_hard_20_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/mnist/hard-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 129s 121ms/step - loss: 0.1745 - acc: 0.8132 - val_loss: 0.0880 - val_acc: 0.8891\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0752 - acc: 0.9001 - val_loss: 0.0641 - val_acc: 0.9105\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 107s 115ms/step - loss: 0.0607 - acc: 0.9136 - val_loss: 0.0551 - val_acc: 0.9185\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0535 - acc: 0.9214 - val_loss: 0.0501 - val_acc: 0.9231\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0494 - acc: 0.9250 - val_loss: 0.0469 - val_acc: 0.9289\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 108s 115ms/step - loss: 0.0461 - acc: 0.9287 - val_loss: 0.0444 - val_acc: 0.9302\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0437 - acc: 0.9315 - val_loss: 0.0421 - val_acc: 0.9340\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0421 - acc: 0.9343 - val_loss: 0.0409 - val_acc: 0.9366\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 108s 115ms/step - loss: 0.0406 - acc: 0.9366 - val_loss: 0.0397 - val_acc: 0.9366\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0394 - acc: 0.9391 - val_loss: 0.0385 - val_acc: 0.9384\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0383 - acc: 0.9404 - val_loss: 0.0371 - val_acc: 0.9414\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 108s 115ms/step - loss: 0.0373 - acc: 0.9411 - val_loss: 0.0371 - val_acc: 0.9408\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0369 - acc: 0.9420 - val_loss: 0.0360 - val_acc: 0.9417\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0360 - acc: 0.9432 - val_loss: 0.0355 - val_acc: 0.9437\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 109s 116ms/step - loss: 0.0354 - acc: 0.9440 - val_loss: 0.0347 - val_acc: 0.9449\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0350 - acc: 0.9440 - val_loss: 0.0345 - val_acc: 0.9443\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0343 - acc: 0.9448 - val_loss: 0.0338 - val_acc: 0.9477\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 108s 115ms/step - loss: 0.0340 - acc: 0.9451 - val_loss: 0.0334 - val_acc: 0.9467\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 110s 117ms/step - loss: 0.0338 - acc: 0.9455 - val_loss: 0.0327 - val_acc: 0.9476\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0331 - acc: 0.9473 - val_loss: 0.0322 - val_acc: 0.9439\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0329 - acc: 0.9478 - val_loss: 0.0321 - val_acc: 0.9472\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0326 - acc: 0.9475 - val_loss: 0.0330 - val_acc: 0.9482\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0320 - acc: 0.9489 - val_loss: 0.0319 - val_acc: 0.9495\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0316 - acc: 0.9494 - val_loss: 0.0314 - val_acc: 0.9489\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 107s 115ms/step - loss: 0.0317 - acc: 0.9482 - val_loss: 0.0312 - val_acc: 0.9504\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0313 - acc: 0.9499 - val_loss: 0.0315 - val_acc: 0.9500\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0313 - acc: 0.9500 - val_loss: 0.0307 - val_acc: 0.9505\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 106s 113ms/step - loss: 0.0306 - acc: 0.9508 - val_loss: 0.0306 - val_acc: 0.9514\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0307 - acc: 0.9508 - val_loss: 0.0309 - val_acc: 0.9486\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 107s 114ms/step - loss: 0.0306 - acc: 0.9508 - val_loss: 0.0300 - val_acc: 0.9512\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/soft-20x50-20230310-111935/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/soft-20x50-20230310-111935/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_20_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/mnist/soft-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 153s 143ms/step - loss: 0.1681 - acc: 0.8169 - val_loss: 0.0798 - val_acc: 0.8873\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 131s 139ms/step - loss: 0.0674 - acc: 0.9004 - val_loss: 0.0573 - val_acc: 0.9108\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 129s 138ms/step - loss: 0.0532 - acc: 0.9153 - val_loss: 0.0493 - val_acc: 0.9212\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 128s 136ms/step - loss: 0.0473 - acc: 0.9233 - val_loss: 0.0453 - val_acc: 0.9238\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 131s 139ms/step - loss: 0.0441 - acc: 0.9273 - val_loss: 0.0421 - val_acc: 0.9290\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 128s 137ms/step - loss: 0.0414 - acc: 0.9314 - val_loss: 0.0397 - val_acc: 0.9345\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 128s 137ms/step - loss: 0.0398 - acc: 0.9343 - val_loss: 0.0395 - val_acc: 0.9351\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0382 - acc: 0.9380 - val_loss: 0.0370 - val_acc: 0.9393\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0369 - acc: 0.9392 - val_loss: 0.0368 - val_acc: 0.9369\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0361 - acc: 0.9405 - val_loss: 0.0355 - val_acc: 0.9419\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0353 - acc: 0.9412 - val_loss: 0.0352 - val_acc: 0.9428\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0348 - acc: 0.9423 - val_loss: 0.0342 - val_acc: 0.9431\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0341 - acc: 0.9432 - val_loss: 0.0343 - val_acc: 0.9421\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0337 - acc: 0.9442 - val_loss: 0.0341 - val_acc: 0.9435\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0331 - acc: 0.9459 - val_loss: 0.0328 - val_acc: 0.9446\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0325 - acc: 0.9472 - val_loss: 0.0323 - val_acc: 0.9467\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 126s 135ms/step - loss: 0.0321 - acc: 0.9471 - val_loss: 0.0323 - val_acc: 0.9474\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 127s 136ms/step - loss: 0.0317 - acc: 0.9478 - val_loss: 0.0318 - val_acc: 0.9466\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0316 - acc: 0.9480 - val_loss: 0.0316 - val_acc: 0.9470\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0315 - acc: 0.9476 - val_loss: 0.0315 - val_acc: 0.9494\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0309 - acc: 0.9493 - val_loss: 0.0307 - val_acc: 0.9492\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0310 - acc: 0.9492 - val_loss: 0.0310 - val_acc: 0.9475\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0306 - acc: 0.9493 - val_loss: 0.0308 - val_acc: 0.9482\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0302 - acc: 0.9509 - val_loss: 0.0302 - val_acc: 0.9495\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0299 - acc: 0.9514 - val_loss: 0.0298 - val_acc: 0.9517\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0300 - acc: 0.9510 - val_loss: 0.0302 - val_acc: 0.9504\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0297 - acc: 0.9517 - val_loss: 0.0298 - val_acc: 0.9526\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0293 - acc: 0.9527 - val_loss: 0.0301 - val_acc: 0.9516\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0292 - acc: 0.9531 - val_loss: 0.0295 - val_acc: 0.9509\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0290 - acc: 0.9524 - val_loss: 0.0293 - val_acc: 0.9523\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/flex-20x50-20230310-121413/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/flex-20x50-20230310-121413/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/mnist/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 143s 138ms/step - loss: 0.1494 - acc: 0.8464 - val_loss: 0.0564 - val_acc: 0.8970\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 122s 130ms/step - loss: 0.0529 - acc: 0.9017 - val_loss: 0.0476 - val_acc: 0.9112\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0465 - acc: 0.9120 - val_loss: 0.0439 - val_acc: 0.9188\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0432 - acc: 0.9174 - val_loss: 0.0420 - val_acc: 0.9224\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0410 - acc: 0.9209 - val_loss: 0.0409 - val_acc: 0.9237\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0395 - acc: 0.9239 - val_loss: 0.0400 - val_acc: 0.9258\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0383 - acc: 0.9262 - val_loss: 0.0395 - val_acc: 0.9273\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0375 - acc: 0.9279 - val_loss: 0.0392 - val_acc: 0.9276\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 122s 130ms/step - loss: 0.0367 - acc: 0.9292 - val_loss: 0.0389 - val_acc: 0.9286\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0362 - acc: 0.9305 - val_loss: 0.0387 - val_acc: 0.9290\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0357 - acc: 0.9315 - val_loss: 0.0385 - val_acc: 0.9302\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0353 - acc: 0.9324 - val_loss: 0.0385 - val_acc: 0.9304\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0349 - acc: 0.9330 - val_loss: 0.0384 - val_acc: 0.9308\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0346 - acc: 0.9337 - val_loss: 0.0384 - val_acc: 0.9301\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 121s 129ms/step - loss: 0.0343 - acc: 0.9345 - val_loss: 0.0384 - val_acc: 0.9302\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0341 - acc: 0.9349 - val_loss: 0.0384 - val_acc: 0.9305\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0339 - acc: 0.9354 - val_loss: 0.0385 - val_acc: 0.9304\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0337 - acc: 0.9357 - val_loss: 0.0384 - val_acc: 0.9300\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0335 - acc: 0.9362 - val_loss: 0.0384 - val_acc: 0.9311\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 124s 132ms/step - loss: 0.0333 - acc: 0.9369 - val_loss: 0.0385 - val_acc: 0.9307\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0332 - acc: 0.9373 - val_loss: 0.0386 - val_acc: 0.9305\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0330 - acc: 0.9377 - val_loss: 0.0386 - val_acc: 0.9304\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0329 - acc: 0.9380 - val_loss: 0.0386 - val_acc: 0.9310\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0328 - acc: 0.9383 - val_loss: 0.0386 - val_acc: 0.9303\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 125s 134ms/step - loss: 0.0327 - acc: 0.9386 - val_loss: 0.0387 - val_acc: 0.9304\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0326 - acc: 0.9387 - val_loss: 0.0387 - val_acc: 0.9307\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 125s 133ms/step - loss: 0.0324 - acc: 0.9387 - val_loss: 0.0387 - val_acc: 0.9306\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 127s 136ms/step - loss: 0.0324 - acc: 0.9395 - val_loss: 0.0389 - val_acc: 0.9305\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 126s 135ms/step - loss: 0.0323 - acc: 0.9396 - val_loss: 0.0389 - val_acc: 0.9309\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0322 - acc: 0.9396 - val_loss: 0.0389 - val_acc: 0.9310\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/hard-100x10-20230310-131846/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/hard-100x10-20230310-131846/model\\assets\n"
     ]
    }
   ],
   "source": [
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_hard_100_10 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/mnist/hard-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 111s 104ms/step - loss: 0.1772 - acc: 0.8057 - val_loss: 0.0899 - val_acc: 0.8839\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 86s 91ms/step - loss: 0.0771 - acc: 0.8931 - val_loss: 0.0668 - val_acc: 0.9051\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 84s 90ms/step - loss: 0.0621 - acc: 0.9088 - val_loss: 0.0562 - val_acc: 0.9187\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 85s 91ms/step - loss: 0.0550 - acc: 0.9175 - val_loss: 0.0511 - val_acc: 0.9229\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0503 - acc: 0.9228 - val_loss: 0.0469 - val_acc: 0.9282\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 87s 93ms/step - loss: 0.0474 - acc: 0.9276 - val_loss: 0.0442 - val_acc: 0.9297\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0448 - acc: 0.9305 - val_loss: 0.0427 - val_acc: 0.9314\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 88s 94ms/step - loss: 0.0432 - acc: 0.9320 - val_loss: 0.0411 - val_acc: 0.9355\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 88s 93ms/step - loss: 0.0415 - acc: 0.9346 - val_loss: 0.0395 - val_acc: 0.9376\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 87s 93ms/step - loss: 0.0404 - acc: 0.9351 - val_loss: 0.0388 - val_acc: 0.9379\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 88s 93ms/step - loss: 0.0393 - acc: 0.9374 - val_loss: 0.0382 - val_acc: 0.9381\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0384 - acc: 0.9388 - val_loss: 0.0372 - val_acc: 0.9389\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 86s 91ms/step - loss: 0.0377 - acc: 0.9392 - val_loss: 0.0365 - val_acc: 0.9401\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 86s 91ms/step - loss: 0.0371 - acc: 0.9403 - val_loss: 0.0356 - val_acc: 0.9431\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0362 - acc: 0.9426 - val_loss: 0.0355 - val_acc: 0.9424\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 89s 95ms/step - loss: 0.0360 - acc: 0.9419 - val_loss: 0.0348 - val_acc: 0.9428\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 88s 93ms/step - loss: 0.0354 - acc: 0.9423 - val_loss: 0.0351 - val_acc: 0.9412\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0348 - acc: 0.9430 - val_loss: 0.0339 - val_acc: 0.9439\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0343 - acc: 0.9451 - val_loss: 0.0344 - val_acc: 0.9449\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0344 - acc: 0.9438 - val_loss: 0.0328 - val_acc: 0.9475\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 85s 91ms/step - loss: 0.0338 - acc: 0.9456 - val_loss: 0.0338 - val_acc: 0.9467\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 86s 91ms/step - loss: 0.0337 - acc: 0.9446 - val_loss: 0.0322 - val_acc: 0.9487\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 85s 91ms/step - loss: 0.0330 - acc: 0.9469 - val_loss: 0.0327 - val_acc: 0.9441\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0327 - acc: 0.9463 - val_loss: 0.0320 - val_acc: 0.9474\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 86s 91ms/step - loss: 0.0327 - acc: 0.9465 - val_loss: 0.0321 - val_acc: 0.9462\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0321 - acc: 0.9475 - val_loss: 0.0318 - val_acc: 0.9465\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0324 - acc: 0.9477 - val_loss: 0.0317 - val_acc: 0.9487\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.0317 - acc: 0.9490 - val_loss: 0.0314 - val_acc: 0.9488\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 87s 93ms/step - loss: 0.0317 - acc: 0.9486 - val_loss: 0.0314 - val_acc: 0.9504\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 87s 92ms/step - loss: 0.0315 - acc: 0.9492 - val_loss: 0.0316 - val_acc: 0.9472\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/soft-100x10-20230310-142137/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/soft-100x10-20230310-142137/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_soft_100_10 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/mnist/soft-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 148s 143ms/step - loss: 0.1703 - acc: 0.8085 - val_loss: 0.0809 - val_acc: 0.8896\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0685 - acc: 0.8951 - val_loss: 0.0593 - val_acc: 0.9057\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 130s 139ms/step - loss: 0.0548 - acc: 0.9107 - val_loss: 0.0500 - val_acc: 0.9193\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 128s 136ms/step - loss: 0.0484 - acc: 0.9201 - val_loss: 0.0460 - val_acc: 0.9213\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 153s 163ms/step - loss: 0.0452 - acc: 0.9229 - val_loss: 0.0437 - val_acc: 0.9262\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0428 - acc: 0.9277 - val_loss: 0.0413 - val_acc: 0.9283\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 128s 136ms/step - loss: 0.0410 - acc: 0.9302 - val_loss: 0.0400 - val_acc: 0.9302\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 128s 136ms/step - loss: 0.0396 - acc: 0.9328 - val_loss: 0.0388 - val_acc: 0.9352\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 128s 136ms/step - loss: 0.0386 - acc: 0.9345 - val_loss: 0.0376 - val_acc: 0.9361\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0379 - acc: 0.9351 - val_loss: 0.0368 - val_acc: 0.9396\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0373 - acc: 0.9364 - val_loss: 0.0367 - val_acc: 0.9407\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0361 - acc: 0.9392 - val_loss: 0.0353 - val_acc: 0.9402\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 126s 135ms/step - loss: 0.0359 - acc: 0.9394 - val_loss: 0.0357 - val_acc: 0.9388\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 129s 137ms/step - loss: 0.0353 - acc: 0.9395 - val_loss: 0.0352 - val_acc: 0.9393\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0346 - acc: 0.9419 - val_loss: 0.0350 - val_acc: 0.9392\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 127s 136ms/step - loss: 0.0341 - acc: 0.9432 - val_loss: 0.0333 - val_acc: 0.9451\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0337 - acc: 0.9438 - val_loss: 0.0338 - val_acc: 0.9411\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0337 - acc: 0.9439 - val_loss: 0.0338 - val_acc: 0.9427\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0329 - acc: 0.9444 - val_loss: 0.0337 - val_acc: 0.9406\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 124s 132ms/step - loss: 0.0329 - acc: 0.9455 - val_loss: 0.0331 - val_acc: 0.9415\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 124s 132ms/step - loss: 0.0329 - acc: 0.9446 - val_loss: 0.0334 - val_acc: 0.9426\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 126s 135ms/step - loss: 0.0321 - acc: 0.9460 - val_loss: 0.0324 - val_acc: 0.9446\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 126s 134ms/step - loss: 0.0320 - acc: 0.9465 - val_loss: 0.0317 - val_acc: 0.9463\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 127s 136ms/step - loss: 0.0318 - acc: 0.9475 - val_loss: 0.0319 - val_acc: 0.9453\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0314 - acc: 0.9476 - val_loss: 0.0327 - val_acc: 0.9451\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 124s 132ms/step - loss: 0.0314 - acc: 0.9485 - val_loss: 0.0325 - val_acc: 0.9452\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 123s 131ms/step - loss: 0.0312 - acc: 0.9484 - val_loss: 0.0319 - val_acc: 0.9428\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 123s 132ms/step - loss: 0.0308 - acc: 0.9488 - val_loss: 0.0315 - val_acc: 0.9465\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 124s 132ms/step - loss: 0.0306 - acc: 0.9502 - val_loss: 0.0321 - val_acc: 0.9461\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 126s 135ms/step - loss: 0.0306 - acc: 0.9497 - val_loss: 0.0313 - val_acc: 0.9472\n",
      "INFO:tensorflow:Assets written to: ./logs/mnist/flex-100x10-20230310-150555/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs/mnist/flex-100x10-20230310-150555/model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/mnist/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

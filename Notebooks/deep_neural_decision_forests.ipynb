{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJdKdLaxNIBI"
      },
      "source": [
        "# Classification with Neural Decision Forests\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/15<br>\n",
        "**Last modified:** 2021/01/15<br>\n",
        "**Description:** How to train differentiable decision trees for end-to-end learning in deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1eYVR0NIBL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example provides an implementation of the\n",
        "[Deep Neural Decision Forest](https://ieeexplore.ieee.org/document/7410529)\n",
        "model introduced by P. Kontschieder et al. for structured data classification.\n",
        "It demonstrates how to build a stochastic and differentiable decision tree model,\n",
        "train it end-to-end, and unify decision trees with deep representation learning.\n",
        "\n",
        "## The dataset\n",
        "\n",
        "This example uses the\n",
        "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
        "provided by the\n",
        "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
        "The task is binary classification\n",
        "to predict whether a person is likely to be making over USD 50,000 a year.\n",
        "\n",
        "The dataset includes 48,842 instances with 14 input features (such as age, work class, education, occupation, and so on): 5 numerical features\n",
        "and 9 categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1e-DwNWNIBM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pzZeAdUhNIBM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oPRFjrrNIBN"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xJ5zEomPNIBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset shape: (32561, 15)\n",
            "Test dataset shape: (16282, 15)\n"
          ]
        }
      ],
      "source": [
        "CSV_HEADER = [\n",
        "    \"age\",\n",
        "    \"workclass\",\n",
        "    \"fnlwgt\",\n",
        "    \"education\",\n",
        "    \"education_num\",\n",
        "    \"marital_status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"gender\",\n",
        "    \"capital_gain\",\n",
        "    \"capital_loss\",\n",
        "    \"hours_per_week\",\n",
        "    \"native_country\",\n",
        "    \"income_bracket\",\n",
        "]\n",
        "\n",
        "train_data_url = (\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        ")\n",
        "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
        "\n",
        "test_data_url = (\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        ")\n",
        "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
        "\n",
        "print(f\"Train dataset shape: {train_data.shape}\")\n",
        "print(f\"Test dataset shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BJ9zoy5NIBO"
      },
      "source": [
        "Remove the first record (because it is not a valid data example) and a trailing\n",
        "'dot' in the class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-N0eT3wiNIBO"
      },
      "outputs": [],
      "source": [
        "test_data = test_data[1:]\n",
        "test_data.income_bracket = test_data.income_bracket.apply(\n",
        "    lambda value: value.replace(\".\", \"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr9Kd4-gNIBP"
      },
      "source": [
        "We store the training and test data splits locally as CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "olcNc0QBNIBP"
      },
      "outputs": [],
      "source": [
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "\n",
        "train_data.to_csv(train_data_file, index=False, header=False)\n",
        "test_data.to_csv(test_data_file, index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bchBGotoNIBP"
      },
      "source": [
        "## Define dataset metadata\n",
        "\n",
        "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
        "and encoding input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3G7O0VeNIBQ"
      },
      "outputs": [],
      "source": [
        "# A list of the numerical feature names.\n",
        "NUMERIC_FEATURE_NAMES = [\n",
        "    \"age\",\n",
        "    \"education_num\",\n",
        "    \"capital_gain\",\n",
        "    \"capital_loss\",\n",
        "    \"hours_per_week\",\n",
        "]\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
        "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
        "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
        "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
        "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
        "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
        "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
        "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
        "}\n",
        "# A list of the columns to ignore from the dataset.\n",
        "IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"income_bracket\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKUUdkOaNIBQ"
      },
      "source": [
        "## Create `tf.data.Dataset` objects for training and validation\n",
        "\n",
        "We create an input function to read and parse the file, and convert features and labels\n",
        "into a [`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
        "for training and validation. We also preprocess the input by mapping the target label\n",
        "to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "irw7Qh56NIBR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\numpy\\core\\numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  return bool(asarray(a1 == a2).all())\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "\n",
        "target_label_lookup = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
        "    return dataset.cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNeT_02XNIBR"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sq2jcE9-NIBR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7A9JiQNIBS"
      },
      "source": [
        "## Encode input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_dQJd0epNIBS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            # Create a lookup to convert a string values to an integer indices.\n",
        "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "            lookup = StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_index = lookup(inputs[feature_name])\n",
        "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = embedding(value_index)\n",
        "        else:\n",
        "            # Use the numerical features as-is.\n",
        "            encoded_feature = inputs[feature_name]\n",
        "            if inputs[feature_name].shape[-1] is None:\n",
        "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HqlUUpeNIBS"
      },
      "source": [
        "## Deep Neural Decision Tree\n",
        "\n",
        "A neural decision tree model has two sets of weights to learn. The first set is `pi`,\n",
        "which represents the probability distribution of the classes in the tree leaves.\n",
        "The second set is the weights of the routing layer `decision_fn`, which represents the probability\n",
        "of going to each leave. The forward pass of the model works as follows:\n",
        "\n",
        "1. The model expects input `features` as a single vector encoding all the features of an instance\n",
        "in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images\n",
        "or dense transformations applied to structured data features.\n",
        "2. The model first applies a `used_features_mask` to randomly select a subset of input features to use.\n",
        "3. Then, the model computes the probabilities (`mu`) for the input instances to reach the tree leaves\n",
        "by iteratively performing a *stochastic* routing throughout the tree levels.\n",
        "4. Finally, the probabilities of reaching the leaves are combined by the class probabilities at the\n",
        "leaves to produce the final `outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QYROWatXNIBS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.num_leaves = 2 ** depth\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        one_hot = np.eye(num_features)\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )  # [batch_size, num_used_features]\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )  # [batch_size, num_leaves, 1]\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )  # [batch_size, num_leaves, 2]\n",
        "\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbWpazDTNIBT"
      },
      "source": [
        "## Deep Neural Decision Forest\n",
        "\n",
        "The neural decision forest model consists of a set of neural decision trees that are\n",
        "trained simultaneously. The output of the forest model is the average outputs of its trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D5AaKzIXNIBT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super().__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Tdvd3nNIBU"
      },
      "source": [
        "Finally, let's set up the code that will train and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vvbIDQ-1NIBU"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 265\n",
        "num_epochs = 10\n",
        "hidden_units = [64, 64]\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        train_data_file, shuffle=True, batch_size=batch_size\n",
        "    )\n",
        "    \n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    print(\"Evaluating the model on the test data...\")\n",
        "    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-IR-xX8NIBU"
      },
      "source": [
        "## Experiment 1: train a decision tree model\n",
        "\n",
        "In this experiment, we train a single neural decision tree model\n",
        "where we use all input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Slae8ZxmNIBU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Barry Qin\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['fnlwgt'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "123/123 [==============================] - 8s 37ms/step - loss: 0.4437 - sparse_categorical_accuracy: 0.8308\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 3s 21ms/step - loss: 0.3357 - sparse_categorical_accuracy: 0.8512\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 3s 21ms/step - loss: 0.3225 - sparse_categorical_accuracy: 0.8533\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 2s 20ms/step - loss: 0.3165 - sparse_categorical_accuracy: 0.8554\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 3s 21ms/step - loss: 0.3121 - sparse_categorical_accuracy: 0.8573\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 2s 19ms/step - loss: 0.3080 - sparse_categorical_accuracy: 0.8596\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 2s 20ms/step - loss: 0.3042 - sparse_categorical_accuracy: 0.8616\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 2s 19ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.8644\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 2s 19ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.8678\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 2s 20ms/step - loss: 0.2910 - sparse_categorical_accuracy: 0.8702\n",
            "Model training finished\n",
            "Evaluating the model on the test data...\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3258 - sparse_categorical_accuracy: 0.8479\n",
            "Test accuracy: 84.79%\n"
          ]
        }
      ],
      "source": [
        "num_trees = 10\n",
        "depth = 10\n",
        "used_features_rate = 1.0\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "\n",
        "def create_tree_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "tree_model = create_tree_model()\n",
        "run_experiment(tree_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KLiqlVXNIBU"
      },
      "source": [
        "## Experiment 2: train a forest model\n",
        "\n",
        "In this experiment, we train a neural decision forest with `num_trees` trees\n",
        "where each tree uses randomly selected 50% of the input features. You can control the number\n",
        "of features to be used in each tree by setting the `used_features_rate` variable.\n",
        "In addition, we set the depth to 5 instead of 10 compared to the previous experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UUe0vmAyNIBV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "123/123 [==============================] - 21s 77ms/step - loss: 0.4570 - sparse_categorical_accuracy: 0.8142\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 9s 73ms/step - loss: 0.3341 - sparse_categorical_accuracy: 0.8516\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 9s 72ms/step - loss: 0.3212 - sparse_categorical_accuracy: 0.8543\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 9s 73ms/step - loss: 0.3160 - sparse_categorical_accuracy: 0.8560\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 9s 74ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.8565\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 9s 74ms/step - loss: 0.3104 - sparse_categorical_accuracy: 0.8574\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 9s 75ms/step - loss: 0.3083 - sparse_categorical_accuracy: 0.8579\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 9s 76ms/step - loss: 0.3065 - sparse_categorical_accuracy: 0.8584\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 9s 74ms/step - loss: 0.3047 - sparse_categorical_accuracy: 0.8592\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 9s 75ms/step - loss: 0.3031 - sparse_categorical_accuracy: 0.8596\n",
            "Model training finished\n",
            "Evaluating the model on the test data...\n",
            "62/62 [==============================] - 3s 25ms/step - loss: 0.3116 - sparse_categorical_accuracy: 0.8567\n",
            "Test accuracy: 85.67%\n"
          ]
        }
      ],
      "source": [
        "num_trees = 25\n",
        "depth = 5\n",
        "used_features_rate = 0.5\n",
        "\n",
        "\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "forest_model = create_forest_model()\n",
        "\n",
        "run_experiment(forest_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlWCqKfXNIBV"
      },
      "source": [
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/neural-decision-forest) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Neural-Decision-Forest)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__encode_inputs() takes 1 positional argument but 2 were given\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misolation_encoding\u001b[39m\u001b[39m\"\u001b[39m, inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m---> 79\u001b[0m ann_model \u001b[39m=\u001b[39m create_ann_model()\n\u001b[0;32m     81\u001b[0m run_experiment(ann_model)\n",
            "Cell \u001b[1;32mIn[14], line 53\u001b[0m, in \u001b[0;36mcreate_ann_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_ann_model\u001b[39m():\n\u001b[0;32m     49\u001b[0m     train_dataset \u001b[39m=\u001b[39m get_dataset_from_csv(\n\u001b[0;32m     50\u001b[0m         train_data_file, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m     51\u001b[0m     )\n\u001b[1;32m---> 53\u001b[0m     t_samples \u001b[39m=\u001b[39m [\n\u001b[0;32m     54\u001b[0m         \u001b[39mlist\u001b[39m(\n\u001b[0;32m     55\u001b[0m             train_dataset\n\u001b[0;32m     56\u001b[0m             \u001b[39m.\u001b[39mshuffle(n)\n\u001b[0;32m     57\u001b[0m             \u001b[39m.\u001b[39mtake(psi)\n\u001b[0;32m     58\u001b[0m             \u001b[39m.\u001b[39mmap(encode_inputs, num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     59\u001b[0m             \u001b[39m.\u001b[39mbatch(psi)\n\u001b[0;32m     60\u001b[0m             \u001b[39m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m     61\u001b[0m         )[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m     62\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(t)\n\u001b[0;32m     63\u001b[0m     ]\n\u001b[0;32m     65\u001b[0m     inputs \u001b[39m=\u001b[39m create_model_inputs()\n\u001b[0;32m     66\u001b[0m     features \u001b[39m=\u001b[39m encode_inputs(inputs)\n",
            "Cell \u001b[1;32mIn[14], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_ann_model\u001b[39m():\n\u001b[0;32m     49\u001b[0m     train_dataset \u001b[39m=\u001b[39m get_dataset_from_csv(\n\u001b[0;32m     50\u001b[0m         train_data_file, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     53\u001b[0m     t_samples \u001b[39m=\u001b[39m [\n\u001b[0;32m     54\u001b[0m         \u001b[39mlist\u001b[39m(\n\u001b[1;32m---> 55\u001b[0m             train_dataset\n\u001b[0;32m     56\u001b[0m             \u001b[39m.\u001b[39;49mshuffle(n)\n\u001b[0;32m     57\u001b[0m             \u001b[39m.\u001b[39;49mtake(psi)\n\u001b[0;32m     58\u001b[0m             \u001b[39m.\u001b[39;49mmap(encode_inputs, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m     59\u001b[0m             \u001b[39m.\u001b[39mbatch(psi)\n\u001b[0;32m     60\u001b[0m             \u001b[39m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m     61\u001b[0m         )[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m     62\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(t)\n\u001b[0;32m     63\u001b[0m     ]\n\u001b[0;32m     65\u001b[0m     inputs \u001b[39m=\u001b[39m create_model_inputs()\n\u001b[0;32m     66\u001b[0m     features \u001b[39m=\u001b[39m encode_inputs(inputs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2204\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2202\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2204\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2205\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2206\u001b[0m       map_func,\n\u001b[0;32m   2207\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2208\u001b[0m       deterministic,\n\u001b[0;32m   2209\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2210\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5441\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m   5440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5442\u001b[0m     map_func,\n\u001b[0;32m   5443\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m   5444\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m   5445\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m   5446\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5447\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2610\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2602\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2603\u001b[0m \n\u001b[0;32m   2604\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2611\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2612\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2576\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2577\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2578\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2579\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2673\u001b[0m         args,\n\u001b[0;32m   2674\u001b[0m         kwargs,\n\u001b[0;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1247\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1247\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1249\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   1252\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    243\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    244\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m    245\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    249\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    176\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 177\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m    179\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__encode_inputs() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "n = train_data.shape[0]\n",
        "psi = 10\n",
        "t = 100\n",
        "soft = True\n",
        "p = 2\n",
        "\n",
        "\n",
        "def _tf_ann(X, samples, p=2, soft=True):\n",
        "    m_dis = None\n",
        "    for i in range(samples.shape[0]):\n",
        "        i_sample = samples[i : i + 1, :]\n",
        "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
        "            1 / p\n",
        "        )\n",
        "        if m_dis is None:\n",
        "            m_dis = l_dis\n",
        "        else:\n",
        "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
        "\n",
        "    if soft:\n",
        "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
        "    else:\n",
        "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
        "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
        "    return feature_map\n",
        "\n",
        "class IsolationEncodingLayer(layers.Layer):\n",
        "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
        "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
        "        self.samples = samples\n",
        "        self.p = p\n",
        "        self.soft = soft\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"samples\": self.samples,\n",
        "                \"p\": self.p,\n",
        "                \"soft\": self.soft,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "    \n",
        "def create_ann_model():\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        train_data_file, shuffle=True, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    t_samples = [\n",
        "        list(\n",
        "            train_dataset\n",
        "            .shuffle(n)\n",
        "            .take(psi)\n",
        "            .map(encode_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .batch(psi)\n",
        "            .as_numpy_iterator()\n",
        "        )[0][0]\n",
        "        for _ in range(t)\n",
        "    ]\n",
        "\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    lambdas = [\n",
        "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
        "            features\n",
        "        )\n",
        "        for i in range(t)\n",
        "    ]\n",
        "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
        "    outputs = layers.Dense(units=num_classes, name=\"outputs_y\")(concatenated)\n",
        "\n",
        "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "ann_model = create_ann_model()\n",
        "\n",
        "run_experiment(ann_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep_neural_decision_forests",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

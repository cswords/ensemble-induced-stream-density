{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "image_w, image_h = 32, 32\n",
    "\n",
    "!rm -rf ./logs/rock_paper_scissors/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    \"rock_paper_scissors\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "n_classes = ds_info.features[\"label\"].num_classes\n",
    "n = ds_info.splits[\"train\"].num_examples\n",
    "\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = layers.Resizing(image_h, image_w)(image)\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(tf.cast(label, tf.int32), n_classes)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "ds_train_normalized = ds_train_raw.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache()\n",
    "\n",
    "ds_test_normalized = ds_test_raw.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache()\n",
    "\n",
    "\n",
    "def prepare(ds, batch_size=batch_size):\n",
    "    return ds.shuffle(n).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dims = list(ds_train_normalized.take(1))[0][0].shape[0]\n",
    "\n",
    "print(\"n: \", n, \"n_classes: \", n_classes, \"dims: \", dims)\n",
    "\n",
    "\n",
    "def minmax_reducer(current, input):\n",
    "    X, _ = input\n",
    "    return (\n",
    "        tf.reduce_min([current[0], X], axis=0),\n",
    "        tf.reduce_max([current[0], X], axis=0),\n",
    "    )\n",
    "\n",
    "\n",
    "x0, _ = list(ds_train_normalized.take(1))[0]\n",
    "min_train, max_train = ds_train_normalized.reduce((x0, x0), minmax_reducer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomFourierFeatures = keras.layers.experimental.RandomFourierFeatures\n",
    "\n",
    "model_svm = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(dims,)),\n",
    "        RandomFourierFeatures(\n",
    "            output_dim=2000, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "        ),\n",
    "        layers.Dense(units=n_classes),\n",
    "    ]\n",
    ")\n",
    "model_svm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.hinge,\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "modeldir = \"./logs/rock_paper_scissors/linear-8192-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_svm.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=modeldir + \"/log\",\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "model_svm.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(psi, t=1000):\n",
    "    return [\n",
    "        list(\n",
    "            ds_train_raw.shuffle(n)\n",
    "            .take(psi)\n",
    "            .map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(psi)\n",
    "            .as_numpy_iterator()\n",
    "        )[0][0]\n",
    "        for _ in range(t)\n",
    "    ]\n",
    "\n",
    "\n",
    "def _tf_ann(X, samples, p=2, soft=True):\n",
    "    m_dis = None\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        )\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, soft=True, **kwargs):\n",
    "        super(IsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "        self.soft = soft\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p, self.soft)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "                \"soft\": self.soft,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_model(t_samples, p=2, soft=True):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        IsolationEncodingLayer(t_samples[i], p=p, soft=soft, name=\"ann_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.dimential_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann(inputs, self.samples, self.p) * self.dimential_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "\n",
    "model_hard_20_50 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/rock_paper_scissors/hard-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_20_50 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/rock_paper_scissors/soft-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/rock_paper_scissors/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_hard_100_10 = build_model(t_samples, soft=False)\n",
    "modeldir = \"./logs/rock_paper_scissors/hard-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_hard_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_hard_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soft_100_10 = build_model(t_samples, soft=True)\n",
    "modeldir = \"./logs/rock_paper_scissors/soft-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_soft_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_soft_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/rock_paper_scissors/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_ann_weighted(X, samples, sample_weights, p=2, soft=True):\n",
    "    m_dis = None # [n, psi]\n",
    "    for i in range(samples.shape[0]):\n",
    "        i_sample = samples[i : i + 1, :] # [i, dims]\n",
    "        l_dis = tf.math.reduce_sum((X - i_sample) ** p, axis=1, keepdims=True) ** (\n",
    "            1 / p\n",
    "        ) # [n, 1]\n",
    "        if m_dis is None:\n",
    "            m_dis = l_dis\n",
    "        else:\n",
    "            m_dis = tf.concat([m_dis, l_dis], 1)\n",
    "\n",
    "    m_dis = m_dis * sample_weights\n",
    "\n",
    "    if soft:\n",
    "        feature_map = tf.nn.softmax(-m_dis, axis=0)\n",
    "    else:\n",
    "        feature_map = tf.one_hot(tf.math.argmax(-m_dis, axis=1), samples.shape[0])\n",
    "    # l_dis_min = tf.math.reduce_sum(m_dis * feature_map, axis=0)\n",
    "    return feature_map\n",
    "\n",
    "class FlexibleIsolationEncodingLayer(layers.Layer):\n",
    "    def __init__(self, samples, p=2, **kwargs):\n",
    "        super(FlexibleIsolationEncodingLayer, self).__init__(**kwargs)\n",
    "        self.samples = samples\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.sample_weights = self.add_weight(\n",
    "            name=\"dimential_weights\",\n",
    "            shape=(\n",
    "                1,\n",
    "                self.samples.shape[0],\n",
    "            ),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FlexibleIsolationEncodingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return _tf_ann_weighted(inputs, self.samples, self.sample_weights, self.p)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"samples\": self.samples,\n",
    "                \"p\": self.p,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_flex_model(t_samples, p=2):\n",
    "    t = len(t_samples)\n",
    "    if t <= 0:\n",
    "        raise ValueError(\"t <= 0\")\n",
    "    _, dims = t_samples[0].shape\n",
    "\n",
    "    inputs = keras.Input(name=\"inputs_x\", shape=(dims,))\n",
    "    lambdas = [\n",
    "        FlexibleIsolationEncodingLayer(t_samples[i], p=p, name=\"ann_flex_{}\".format(i))(\n",
    "            inputs\n",
    "        )\n",
    "        for i in range(t)\n",
    "    ]\n",
    "    concatenated = layers.Concatenate(axis=1, name=\"concatenated\")(lambdas)\n",
    "    outputs = layers.Dense(units=n_classes, name=\"outputs_y\")(concatenated)\n",
    "\n",
    "    model = keras.Model(name=\"isolation_encoding\", inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.hinge,\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "t_samples = gen_samples(psi=100, t=10)\n",
    "\n",
    "model_flex_100_10 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/mnist/flex-100x10-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_100_10.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_100_10.save(modeldir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = gen_samples(psi=20, t=50)\n",
    "\n",
    "model_flex_20_50 = build_flex_model(t_samples)\n",
    "# model_flex_20_50.summary()\n",
    "modeldir = \"./logs/mnist/flex-20x50-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_flex_20_50.fit(\n",
    "    prepare(ds_train_normalized),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=prepare(ds_test_normalized),\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(log_dir=modeldir + \"/log\", histogram_freq=1)\n",
    "    ],\n",
    ")\n",
    "model_flex_20_50.save(modeldir + \"/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a47f39fd070b48c46d7ad468a6f203b63097621f5a6c21be0934a2bf61a8c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
